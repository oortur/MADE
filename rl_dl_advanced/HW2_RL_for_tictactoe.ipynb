{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "HW2_RL_for_tictactoe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLsr-fR_CTGX"
      },
      "source": [
        "# RL and Advanced DL. Homework #2\n",
        "<br>\n",
        "\n",
        "Task text:\n",
        "<br>\n",
        "\n",
        "https://docs.google.com/document/d/1laNIbABgIdjLiwHbd0sl0l_A4qGZcuQaOlQNrza82bQ\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mDvvYQg6cL6"
      },
      "source": [
        "import gym\n",
        "\n",
        "import math\n",
        "import random\n",
        "from datetime import datetime\n",
        "# import pickle\n",
        "# import multiprocessing\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.autograd import Variable\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"colorblind\")\n",
        "palette = sns.color_palette()\n",
        "figsize = (17, 8)\n",
        "legend_fontsize = 16"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YEjygIoEvcy"
      },
      "source": [
        "We start with definition of environment for Tic-tac-toe (with arbitrary biard size and number of elements in line to win)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxhVeFoTFLJ0"
      },
      "source": [
        "N_ROWS, N_COLS, N_WIN = 3, 3, 3\n",
        "\n",
        "class TicTacToe(gym.Env):\n",
        "    def __init__(self, n_rows=N_ROWS, n_cols=N_COLS, n_win=N_WIN, clone=None):\n",
        "        if clone is not None:\n",
        "            self.n_rows, self.n_cols, self.n_win = clone.n_rows, clone.n_cols, clone.n_win\n",
        "            self.board = copy.deepcopy(clone.board)\n",
        "            self.curTurn = clone.curTurn\n",
        "            self.emptySpaces = None\n",
        "            self.boardHash = None\n",
        "        else:\n",
        "            self.n_rows = n_rows\n",
        "            self.n_cols = n_cols\n",
        "            self.n_win = n_win\n",
        "\n",
        "            self.reset()\n",
        "\n",
        "    def getEmptySpaces(self):\n",
        "        if self.emptySpaces is None:\n",
        "            res = np.where(self.board == 0)\n",
        "            self.emptySpaces = np.array([ (i, j) for i,j in zip(res[0], res[1]) ])\n",
        "        return self.emptySpaces\n",
        "\n",
        "    def makeMove(self, player, i, j):\n",
        "        self.board[i, j] = player\n",
        "        self.emptySpaces = None\n",
        "        self.boardHash = None\n",
        "\n",
        "    def getHash(self):\n",
        "        if self.boardHash is None:\n",
        "            self.boardHash = ''.join(['%s' % (x+1) for x in self.board.reshape(self.n_rows * self.n_cols)])\n",
        "        return self.boardHash\n",
        "\n",
        "    def isTerminal(self):\n",
        "        # проверим, не закончилась ли игра\n",
        "        cur_marks, cur_p = np.where(self.board == self.curTurn), self.curTurn\n",
        "        for i,j in zip(cur_marks[0], cur_marks[1]):\n",
        "            win = False\n",
        "            if i <= self.n_rows - self.n_win:\n",
        "                if np.all(self.board[i:i+self.n_win, j] == cur_p):\n",
        "                    win = True\n",
        "            if not win:\n",
        "                if j <= self.n_cols - self.n_win:\n",
        "                    if np.all(self.board[i,j:j+self.n_win] == cur_p):\n",
        "                        win = True\n",
        "            if not win:\n",
        "                if i <= self.n_rows - self.n_win and j <= self.n_cols - self.n_win:\n",
        "                    if np.all(np.array([ self.board[i+k,j+k] == cur_p for k in range(self.n_win) ])):\n",
        "                        win = True\n",
        "            if not win:\n",
        "                if i <= self.n_rows - self.n_win and j >= self.n_win-1:\n",
        "                    if np.all(np.array([ self.board[i+k,j-k] == cur_p for k in range(self.n_win) ])):\n",
        "                        win = True\n",
        "            if win:\n",
        "                self.gameOver = True\n",
        "                return self.curTurn\n",
        "\n",
        "        if len(self.getEmptySpaces()) == 0:\n",
        "            self.gameOver = True\n",
        "            return 0\n",
        "\n",
        "        self.gameOver = False\n",
        "        return None\n",
        "\n",
        "    def printBoard(self):\n",
        "        for i in range(0, self.n_rows):\n",
        "            print('----'*(self.n_cols)+'-')\n",
        "            out = '| '\n",
        "            for j in range(0, self.n_cols):\n",
        "                if self.board[i, j] == 1:\n",
        "                    token = 'x'\n",
        "                if self.board[i, j] == -1:\n",
        "                    token = 'o'\n",
        "                if self.board[i, j] == 0:\n",
        "                    token = ' '\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('----'*(self.n_cols)+'-')\n",
        "\n",
        "    def getState(self):\n",
        "        return (self.getHash(), self.getEmptySpaces(), self.curTurn)\n",
        "\n",
        "    def action_from_int(self, action_int):\n",
        "        return ( int(action_int / self.n_cols), int(action_int % self.n_cols))\n",
        "\n",
        "    def int_from_action(self, action):\n",
        "        return action[0] * self.n_cols + action[1]\n",
        "    \n",
        "    def step(self, action):\n",
        "        if self.board[action[0], action[1]] != 0:\n",
        "            return self.getState(), -10, True, {}\n",
        "        self.makeMove(self.curTurn, action[0], action[1])\n",
        "        reward = self.isTerminal()\n",
        "        self.curTurn = -self.curTurn\n",
        "        return self.getState(), 0 if reward is None else reward, reward is not None, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
        "        self.boardHash = None\n",
        "        self.gameOver = False\n",
        "        self.emptySpaces = None\n",
        "        self.curTurn = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq7Me2PGI11I"
      },
      "source": [
        "def plot_board(env, pi, showtext=True, verbose=True, fontq=20, fontx=60):\n",
        "    '''Рисуем доску с оценками из стратегии pi'''\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "    X, Y = np.meshgrid(np.arange(0, env.n_rows), np.arange(0, env.n_rows))\n",
        "    Z = np.zeros((env.n_rows, env.n_cols)) + .01\n",
        "    s, actions = env.getHash(), env.getEmptySpaces()\n",
        "    if pi is not None and s in pi.Q:\n",
        "        for i, a in enumerate(actions):\n",
        "            Z[a[0], a[1]] = pi.Q[s][i]\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    surf = ax.imshow(Z, cmap=plt.get_cmap('Accent', 10), vmin=-1, vmax=1)\n",
        "    if showtext:\n",
        "        for i,a in enumerate(actions):\n",
        "            if pi is not None and s in pi.Q:\n",
        "                ax.text( a[1] , a[0] , \"%.3f\" % pi.Q[s][i], fontsize=fontq, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
        "    for i in range(env.n_rows):\n",
        "        for j in range(env.n_cols):\n",
        "            if env.board[i, j] == -1:\n",
        "                ax.text(j, i, \"O\", fontsize=fontx, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
        "            if env.board[i, j] == 1:\n",
        "                ax.text(j, i, \"X\", fontsize=fontx, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
        "#     cbar = plt.colorbar(surf, ticks=[0, 1])\n",
        "    ax.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "def get_and_print_move(env, pi, s, actions, random=False, verbose=True, fontq=20, fontx=60):\n",
        "    '''Делаем ход, рисуем доску'''\n",
        "    plot_board(env, pi, fontq=fontq, fontx=fontx)\n",
        "    if verbose and (pi is not None):\n",
        "        if s in pi.Q:\n",
        "            for i,a in enumerate(actions):\n",
        "                print(i, a, pi.Q[s][i])\n",
        "        else:\n",
        "            print(\"Стратегия не знает, что делать...\")\n",
        "    if random:\n",
        "        return np.random.randint(len(actions))\n",
        "    else:\n",
        "        return pi.getActionGreedy(s, len(actions))\n",
        "\n",
        "def plot_test_game(env, pi1, pi2, random_crosses=False, random_naughts=True, verbose=True, fontq=20, fontx=60):\n",
        "    '''Играем тестовую партию между стратегиями или со случайными ходами, рисуем ход игры'''\n",
        "    done = False\n",
        "    env.reset()\n",
        "    while not done:\n",
        "        s, actions = env.getHash(), env.getEmptySpaces()\n",
        "        if env.curTurn == 1:\n",
        "            a = get_and_print_move(env, pi1, s, actions, random=random_crosses, verbose=verbose, fontq=fontq, fontx=fontx)\n",
        "        else:\n",
        "            a = get_and_print_move(env, pi2, s, actions, random=random_naughts, verbose=verbose, fontq=fontq, fontx=fontx)\n",
        "        observation, reward, done, info = env.step(actions[a])\n",
        "        if reward == 1:\n",
        "            print(\"Крестики выиграли!\")\n",
        "            plot_board(env, None, showtext=False, fontq=fontq, fontx=fontx)\n",
        "        if reward == -1:\n",
        "            print(\"Нолики выиграли!\")\n",
        "            plot_board(env, None, showtext=False, fontq=fontq, fontx=fontx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSXMLFSCDlsa"
      },
      "source": [
        "# I. Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJu5W6mZEfbF"
      },
      "source": [
        "The first method to try is classic table Q-learning.\n",
        "\n",
        "The idea is to learn simultaneously both Q-functions for crosses and noughts. The algorithm is:\n",
        "1. initialize Q1 and Q2 (for crosses and noughts respectively)\n",
        "2. for given number of episodes run Q-learning for both players (as far as players act one after another, we update Q-functions in turns)\n",
        "\n",
        "*Note.* Everywhere after in code I use following notation:\n",
        "- `s` - current state\n",
        "- `a` - current action\n",
        "- `r` - reward\n",
        "- `s_` - next state\n",
        "- `a_` - next action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "husevL0HFpd1"
      },
      "source": [
        "MAX_CARD_SCORE = 11\n",
        "N_ACTIONS = 2\n",
        "\n",
        "\n",
        "def init_Q():\n",
        "    Q = {}\n",
        "    for player_sum in range(MAX_PLAYER_SUM):\n",
        "        for dealer_card in range(MAX_CARD_SCORE):\n",
        "            for usable_ace in range(2):\n",
        "                Q[(player_sum, dealer_card, usable_ace)] = np.random.randn(N_ACTIONS) if val is None else val * np.ones(N_ACTIONS)\n",
        "    return Q\n",
        "\n",
        "\n",
        "def compute_policy_by_Q(Q):\n",
        "    pi = {}\n",
        "    for s in Q:\n",
        "        pi[s] = np.argmax(Q[s])\n",
        "    return pi\n",
        "\n",
        "\n",
        "def Q_learning_episode(env, pi, Q, alpha=0.05, temperature=0.0, gamma=0.9):\n",
        "    s = env.reset()\n",
        "    a = pi[s] if np.random.rand() > temperature else np.random.randint(N_ACTIONS)\n",
        "    finished = False\n",
        "    while not finished:\n",
        "        s_, r, finished, _ = env.step(a)\n",
        "        a_ = pi[s_] if np.random.rand() > temperature else np.random.randint(N_ACTIONS)\n",
        "        Q[s][a] = Q[s][a] + alpha * (r + gamma * np.max(Q[s_]) - Q[s][a])\n",
        "        s, a = s_, a_\n",
        "    return Q, r\n",
        "\n",
        "def Q_learning(env, n_episodes, alpha=0.1, temperature=0.0, gamma=0.95, verbose=False):\n",
        "    sum_reward = 0\n",
        "    mean_rewards = []\n",
        "    Q = init_Q()\n",
        "    pi = compute_policy_by_Q(Q)\n",
        "    for cur_episode in tqdm(range(n_episodes), disable=not verbose):\n",
        "        Q, r = Q_learning_episode(env, pi, Q, alpha=alpha, temperature=temperature, gamma=gamma)\n",
        "        pi = compute_policy_by_Q(Q)\n",
        "        sum_reward += r\n",
        "        mean_rewards.append(sum_reward / (cur_episode + 1))\n",
        "    return pi, Q, mean_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbcWDJQ6F_t8"
      },
      "source": [
        "def plot_durations(xs, labels):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.xlabel('Episode number')\n",
        "    plt.ylabel('Number of steps')\n",
        "    for i,x in enumerate(xs):\n",
        "        plt.plot(x, label=labels[i])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "# def plot_alg(array, alg_name, cut_first=10):\n",
        "#     plt.figure(figsize=(18,8))\n",
        "#     plt.title(f\"{alg_name}: Mean reward for first N episodes\", fontsize=20)\n",
        "#     plt.xlabel(\"Episodes\", fontsize=18)\n",
        "#     plt.ylabel(\"Mean reward\", fontsize=18)\n",
        "#     plt.plot(array[cut_first:]);\n",
        "\n",
        "# def plot_by_param(reward_arrays, params, param_name, alg_name, cut_first=10):\n",
        "#     plt.figure(figsize=(18,9))\n",
        "#     plt.title(f\"{alg_name}: Mean reward for first N episodes\", fontsize=20)\n",
        "#     plt.xlabel(\"Episodes\", fontsize=18)\n",
        "#     plt.ylabel(\"Mean reward\", fontsize=18)\n",
        "#     assert len(reward_arrays) == len(params)\n",
        "#     for array, param in zip(reward_arrays, params):\n",
        "#         plt.plot(array[cut_first:], label=f\"{param_name} = {param}\")\n",
        "#     plt.legend(fontsize=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QlSAG02IaLv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HE5rKPYEFrU"
      },
      "source": [
        "# II. DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUGu-PUc6cMq"
      },
      "source": [
        "class ReplayMemory():\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def store(self, exptuple):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = exptuple\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "       \n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5U8vdo96cM1"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, layer_size=256):\n",
        "        nn.Module.__init__(self)\n",
        "        self.l1 = nn.Linear(4, layer_size)\n",
        "        self.l2 = nn.Linear(layer_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = self.l2(x)\n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1xnF3J3IEZ1"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, n_channels=100, n_board=3):\n",
        "        super().__init__()\n",
        "        self.n_board = n_board\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=3, padding=0)\n",
        "        self.fc = nn.Linear(in_features=n_channels, out_features=n_board*n_board)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # (bs, n_board, n_board)\n",
        "        x = self.conv(x.unsqueeze(1))\n",
        "        x = torch.amax(x, dim=(2, 3))\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Umzk-Er6caw"
      },
      "source": [
        "class TictactoeDQN():\n",
        "    def __init__(self):\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        self.model = Network()\n",
        "        self.memory = ReplayMemory(10000)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), 0.001)\n",
        "        self.steps_done = 0\n",
        "        self.episode_durations = []\n",
        "        \n",
        "        self.gamma = 0.8\n",
        "        self.batch_size = 64\n",
        "        \n",
        "        self.eps_init, self.eps_final, self.eps_decay = 0.9, 0.05, 200\n",
        "        self.num_step = 0\n",
        "\n",
        "    def select_greedy_action(self, state):\n",
        "        return self.model(state).data.max(1)[1].view(1, 1)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        sample = random.random()\n",
        "        self.num_step += 1\n",
        "        eps_threshold = self.eps_final + (self.eps_init - self.eps_final) * math.exp(-1. * self.num_step / self.eps_decay)\n",
        "        if sample > eps_threshold:\n",
        "            return self.select_greedy_action(state)\n",
        "        else:\n",
        "            return torch.tensor([[random.randrange(2)]], dtype=torch.int64)\n",
        "        \n",
        "    def run_episode(self, e=0, do_learning=True, greedy=False, render=False):\n",
        "        state, num_step = self.env.reset(), 0\n",
        "        while True:\n",
        "            if render:\n",
        "                self.env.render()\n",
        "\n",
        "            state_tensor = torch.tensor([state], dtype=torch.float32)\n",
        "            with torch.no_grad():\n",
        "                if greedy:\n",
        "                    action = self.select_greedy_action(state_tensor)\n",
        "                else:\n",
        "                    action = self.select_action(state_tensor)\n",
        "            next_state, reward, done, _ = self.env.step(action.numpy()[0][0])\n",
        "            next_state_tensor = torch.tensor([next_state], dtype=torch.float32)\n",
        "\n",
        "            if done:\n",
        "                reward = -1\n",
        "\n",
        "            transition = (state_tensor, action, next_state_tensor, torch.tensor([reward], dtype=torch.float32))\n",
        "            self.memory.store(transition)\n",
        "\n",
        "            if do_learning:\n",
        "                self.learn()\n",
        "\n",
        "            state = next_state\n",
        "            num_step += 1\n",
        "\n",
        "            if done:\n",
        "                print(\"\\tepisode %d finished after %d steps\" % (e, num_step))\n",
        "                self.episode_durations.append(num_step)\n",
        "                break\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # берём мини-батч из памяти\n",
        "        transitions = self.memory.sample(self.batch_size)\n",
        "        batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
        "\n",
        "        batch_state = Variable(torch.cat(batch_state))\n",
        "        batch_action = Variable(torch.cat(batch_action))\n",
        "        batch_reward = Variable(torch.cat(batch_reward))\n",
        "        batch_next_state = Variable(torch.cat(batch_next_state))\n",
        "\n",
        "        # считаем значения функции Q\n",
        "        Q = self.model(batch_state).gather(1, batch_action).reshape([self.batch_size])\n",
        "\n",
        "        # оцениваем ожидаемые значения после этого действия\n",
        "        Qmax = self.model(batch_next_state).detach().max(1)[0]\n",
        "        Qnext = batch_reward + (self.gamma * Qmax)\n",
        "\n",
        "        # и хотим, чтобы Q было похоже на Qnext -- это и есть суть Q-обучения\n",
        "        loss = F.smooth_l1_loss(Q, Qnext)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LryTIkVa6ca4",
        "outputId": "e5e10d98-95d5-4659-f712-991288adf3e0"
      },
      "source": [
        "dqn = TictactoeDQN()\n",
        "\n",
        "print(\"%s\\tStarting training for 300 episodes...\" % (datetime.now().time()))\n",
        "for e in range(300):\n",
        "    dqn.run_episode(e)\n",
        "print(\"%s\\t\\t...done!\" % (datetime.now().time()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14:05:14.544354\tStarting training for 300 episodes...\n",
            "\tepisode 0 finished after 11 steps\n",
            "\tepisode 1 finished after 17 steps\n",
            "\tepisode 2 finished after 42 steps\n",
            "\tepisode 3 finished after 17 steps\n",
            "\tepisode 4 finished after 15 steps\n",
            "\tepisode 5 finished after 13 steps\n",
            "\tepisode 6 finished after 12 steps\n",
            "\tepisode 7 finished after 14 steps\n",
            "\tepisode 8 finished after 12 steps\n",
            "\tepisode 9 finished after 10 steps\n",
            "\tepisode 10 finished after 9 steps\n",
            "\tepisode 11 finished after 16 steps\n",
            "\tepisode 12 finished after 10 steps\n",
            "\tepisode 13 finished after 11 steps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tepisode 14 finished after 12 steps\n",
            "\tepisode 15 finished after 11 steps\n",
            "\tepisode 16 finished after 11 steps\n",
            "\tepisode 17 finished after 20 steps\n",
            "\tepisode 18 finished after 9 steps\n",
            "\tepisode 19 finished after 9 steps\n",
            "\tepisode 20 finished after 12 steps\n",
            "\tepisode 21 finished after 23 steps\n",
            "\tepisode 22 finished after 64 steps\n",
            "\tepisode 23 finished after 12 steps\n",
            "\tepisode 24 finished after 26 steps\n",
            "\tepisode 25 finished after 21 steps\n",
            "\tepisode 26 finished after 24 steps\n",
            "\tepisode 27 finished after 23 steps\n",
            "\tepisode 28 finished after 65 steps\n",
            "\tepisode 29 finished after 24 steps\n",
            "\tepisode 30 finished after 43 steps\n",
            "\tepisode 31 finished after 34 steps\n",
            "\tepisode 32 finished after 55 steps\n",
            "\tepisode 33 finished after 87 steps\n",
            "\tepisode 34 finished after 41 steps\n",
            "\tepisode 35 finished after 42 steps\n",
            "\tepisode 36 finished after 145 steps\n",
            "\tepisode 37 finished after 42 steps\n",
            "\tepisode 38 finished after 46 steps\n",
            "\tepisode 39 finished after 27 steps\n",
            "\tepisode 40 finished after 59 steps\n",
            "\tepisode 41 finished after 59 steps\n",
            "\tepisode 42 finished after 109 steps\n",
            "\tepisode 43 finished after 61 steps\n",
            "\tepisode 44 finished after 109 steps\n",
            "\tepisode 45 finished after 128 steps\n",
            "\tepisode 46 finished after 106 steps\n",
            "\tepisode 47 finished after 89 steps\n",
            "\tepisode 48 finished after 114 steps\n",
            "\tepisode 49 finished after 158 steps\n",
            "\tepisode 50 finished after 162 steps\n",
            "\tepisode 51 finished after 144 steps\n",
            "\tepisode 52 finished after 165 steps\n",
            "\tepisode 53 finished after 165 steps\n",
            "\tepisode 54 finished after 163 steps\n",
            "\tepisode 55 finished after 154 steps\n",
            "\tepisode 56 finished after 170 steps\n",
            "\tepisode 57 finished after 177 steps\n",
            "\tepisode 58 finished after 149 steps\n",
            "\tepisode 59 finished after 164 steps\n",
            "\tepisode 60 finished after 190 steps\n",
            "\tepisode 61 finished after 147 steps\n",
            "\tepisode 62 finished after 175 steps\n",
            "\tepisode 63 finished after 162 steps\n",
            "\tepisode 64 finished after 166 steps\n",
            "\tepisode 65 finished after 167 steps\n",
            "\tepisode 66 finished after 200 steps\n",
            "\tepisode 67 finished after 163 steps\n",
            "\tepisode 68 finished after 166 steps\n",
            "\tepisode 69 finished after 147 steps\n",
            "\tepisode 70 finished after 154 steps\n",
            "\tepisode 71 finished after 172 steps\n",
            "\tepisode 72 finished after 175 steps\n",
            "\tepisode 73 finished after 200 steps\n",
            "\tepisode 74 finished after 148 steps\n",
            "\tepisode 75 finished after 200 steps\n",
            "\tepisode 76 finished after 164 steps\n",
            "\tepisode 77 finished after 160 steps\n",
            "\tepisode 78 finished after 179 steps\n",
            "\tepisode 79 finished after 155 steps\n",
            "\tepisode 80 finished after 177 steps\n",
            "\tepisode 81 finished after 158 steps\n",
            "\tepisode 82 finished after 189 steps\n",
            "\tepisode 83 finished after 200 steps\n",
            "\tepisode 84 finished after 153 steps\n",
            "\tepisode 85 finished after 156 steps\n",
            "\tepisode 86 finished after 200 steps\n",
            "\tepisode 87 finished after 175 steps\n",
            "\tepisode 88 finished after 187 steps\n",
            "\tepisode 89 finished after 167 steps\n",
            "\tepisode 90 finished after 184 steps\n",
            "\tepisode 91 finished after 170 steps\n",
            "\tepisode 92 finished after 150 steps\n",
            "\tepisode 93 finished after 154 steps\n",
            "\tepisode 94 finished after 173 steps\n",
            "\tepisode 95 finished after 167 steps\n",
            "\tepisode 96 finished after 172 steps\n",
            "\tepisode 97 finished after 177 steps\n",
            "\tepisode 98 finished after 153 steps\n",
            "\tepisode 99 finished after 187 steps\n",
            "\tepisode 100 finished after 170 steps\n",
            "\tepisode 101 finished after 170 steps\n",
            "\tepisode 102 finished after 200 steps\n",
            "\tepisode 103 finished after 183 steps\n",
            "\tepisode 104 finished after 161 steps\n",
            "\tepisode 105 finished after 168 steps\n",
            "\tepisode 106 finished after 169 steps\n",
            "\tepisode 107 finished after 192 steps\n",
            "\tepisode 108 finished after 173 steps\n",
            "\tepisode 109 finished after 168 steps\n",
            "\tepisode 110 finished after 182 steps\n",
            "\tepisode 111 finished after 184 steps\n",
            "\tepisode 112 finished after 200 steps\n",
            "\tepisode 113 finished after 200 steps\n",
            "\tepisode 114 finished after 173 steps\n",
            "\tepisode 115 finished after 172 steps\n",
            "\tepisode 116 finished after 152 steps\n",
            "\tepisode 117 finished after 200 steps\n",
            "\tepisode 118 finished after 186 steps\n",
            "\tepisode 119 finished after 200 steps\n",
            "\tepisode 120 finished after 200 steps\n",
            "\tepisode 121 finished after 196 steps\n",
            "\tepisode 122 finished after 192 steps\n",
            "\tepisode 123 finished after 200 steps\n",
            "\tepisode 124 finished after 200 steps\n",
            "\tepisode 125 finished after 191 steps\n",
            "\tepisode 126 finished after 200 steps\n",
            "\tepisode 127 finished after 200 steps\n",
            "\tepisode 128 finished after 185 steps\n",
            "\tepisode 129 finished after 141 steps\n",
            "\tepisode 130 finished after 200 steps\n",
            "\tepisode 131 finished after 200 steps\n",
            "\tepisode 132 finished after 163 steps\n",
            "\tepisode 133 finished after 200 steps\n",
            "\tepisode 134 finished after 200 steps\n",
            "\tepisode 135 finished after 172 steps\n",
            "\tepisode 136 finished after 191 steps\n",
            "\tepisode 137 finished after 132 steps\n",
            "\tepisode 138 finished after 200 steps\n",
            "\tepisode 139 finished after 156 steps\n",
            "\tepisode 140 finished after 200 steps\n",
            "\tepisode 141 finished after 200 steps\n",
            "\tepisode 142 finished after 14 steps\n",
            "\tepisode 143 finished after 158 steps\n",
            "\tepisode 144 finished after 174 steps\n",
            "\tepisode 145 finished after 144 steps\n",
            "\tepisode 146 finished after 200 steps\n",
            "\tepisode 147 finished after 200 steps\n",
            "\tepisode 148 finished after 200 steps\n",
            "\tepisode 149 finished after 140 steps\n",
            "\tepisode 150 finished after 62 steps\n",
            "\tepisode 151 finished after 58 steps\n",
            "\tepisode 152 finished after 190 steps\n",
            "\tepisode 153 finished after 83 steps\n",
            "\tepisode 154 finished after 128 steps\n",
            "\tepisode 155 finished after 200 steps\n",
            "\tepisode 156 finished after 200 steps\n",
            "\tepisode 157 finished after 200 steps\n",
            "\tepisode 158 finished after 200 steps\n",
            "\tepisode 159 finished after 200 steps\n",
            "\tepisode 160 finished after 142 steps\n",
            "\tepisode 161 finished after 200 steps\n",
            "\tepisode 162 finished after 128 steps\n",
            "\tepisode 163 finished after 78 steps\n",
            "\tepisode 164 finished after 182 steps\n",
            "\tepisode 165 finished after 174 steps\n",
            "\tepisode 166 finished after 183 steps\n",
            "\tepisode 167 finished after 200 steps\n",
            "\tepisode 168 finished after 147 steps\n",
            "\tepisode 169 finished after 12 steps\n",
            "\tepisode 170 finished after 200 steps\n",
            "\tepisode 171 finished after 173 steps\n",
            "\tepisode 172 finished after 200 steps\n",
            "\tepisode 173 finished after 200 steps\n",
            "\tepisode 174 finished after 200 steps\n",
            "\tepisode 175 finished after 200 steps\n",
            "\tepisode 176 finished after 155 steps\n",
            "\tepisode 177 finished after 200 steps\n",
            "\tepisode 178 finished after 146 steps\n",
            "\tepisode 179 finished after 200 steps\n",
            "\tepisode 180 finished after 200 steps\n",
            "\tepisode 181 finished after 197 steps\n",
            "\tepisode 182 finished after 200 steps\n",
            "\tepisode 183 finished after 179 steps\n",
            "\tepisode 184 finished after 200 steps\n",
            "\tepisode 185 finished after 200 steps\n",
            "\tepisode 186 finished after 169 steps\n",
            "\tepisode 187 finished after 200 steps\n",
            "\tepisode 188 finished after 200 steps\n",
            "\tepisode 189 finished after 200 steps\n",
            "\tepisode 190 finished after 200 steps\n",
            "\tepisode 191 finished after 200 steps\n",
            "\tepisode 192 finished after 29 steps\n",
            "\tepisode 193 finished after 200 steps\n",
            "\tepisode 194 finished after 200 steps\n",
            "\tepisode 195 finished after 96 steps\n",
            "\tepisode 196 finished after 200 steps\n",
            "\tepisode 197 finished after 200 steps\n",
            "\tepisode 198 finished after 200 steps\n",
            "\tepisode 199 finished after 173 steps\n",
            "\tepisode 200 finished after 170 steps\n",
            "\tepisode 201 finished after 200 steps\n",
            "\tepisode 202 finished after 86 steps\n",
            "\tepisode 203 finished after 186 steps\n",
            "\tepisode 204 finished after 200 steps\n",
            "\tepisode 205 finished after 200 steps\n",
            "\tepisode 206 finished after 200 steps\n",
            "\tepisode 207 finished after 200 steps\n",
            "\tepisode 208 finished after 200 steps\n",
            "\tepisode 209 finished after 200 steps\n",
            "\tepisode 210 finished after 11 steps\n",
            "\tepisode 211 finished after 200 steps\n",
            "\tepisode 212 finished after 200 steps\n",
            "\tepisode 213 finished after 200 steps\n",
            "\tepisode 214 finished after 200 steps\n",
            "\tepisode 215 finished after 200 steps\n",
            "\tepisode 216 finished after 200 steps\n",
            "\tepisode 217 finished after 200 steps\n",
            "\tepisode 218 finished after 27 steps\n",
            "\tepisode 219 finished after 156 steps\n",
            "\tepisode 220 finished after 200 steps\n",
            "\tepisode 221 finished after 200 steps\n",
            "\tepisode 222 finished after 200 steps\n",
            "\tepisode 223 finished after 200 steps\n",
            "\tepisode 224 finished after 200 steps\n",
            "\tepisode 225 finished after 200 steps\n",
            "\tepisode 226 finished after 200 steps\n",
            "\tepisode 227 finished after 200 steps\n",
            "\tepisode 228 finished after 200 steps\n",
            "\tepisode 229 finished after 74 steps\n",
            "\tepisode 230 finished after 200 steps\n",
            "\tepisode 231 finished after 200 steps\n",
            "\tepisode 232 finished after 200 steps\n",
            "\tepisode 233 finished after 159 steps\n",
            "\tepisode 234 finished after 64 steps\n",
            "\tepisode 235 finished after 173 steps\n",
            "\tepisode 236 finished after 200 steps\n",
            "\tepisode 237 finished after 200 steps\n",
            "\tepisode 238 finished after 154 steps\n",
            "\tepisode 239 finished after 200 steps\n",
            "\tepisode 240 finished after 192 steps\n",
            "\tepisode 241 finished after 116 steps\n",
            "\tepisode 242 finished after 200 steps\n",
            "\tepisode 243 finished after 200 steps\n",
            "\tepisode 244 finished after 175 steps\n",
            "\tepisode 245 finished after 165 steps\n",
            "\tepisode 246 finished after 15 steps\n",
            "\tepisode 247 finished after 194 steps\n",
            "\tepisode 248 finished after 200 steps\n",
            "\tepisode 249 finished after 186 steps\n",
            "\tepisode 250 finished after 200 steps\n",
            "\tepisode 251 finished after 71 steps\n",
            "\tepisode 252 finished after 200 steps\n",
            "\tepisode 253 finished after 149 steps\n",
            "\tepisode 254 finished after 200 steps\n",
            "\tepisode 255 finished after 200 steps\n",
            "\tepisode 256 finished after 190 steps\n",
            "\tepisode 257 finished after 200 steps\n",
            "\tepisode 258 finished after 156 steps\n",
            "\tepisode 259 finished after 139 steps\n",
            "\tepisode 260 finished after 155 steps\n",
            "\tepisode 261 finished after 154 steps\n",
            "\tepisode 262 finished after 143 steps\n",
            "\tepisode 263 finished after 165 steps\n",
            "\tepisode 264 finished after 183 steps\n",
            "\tepisode 265 finished after 38 steps\n",
            "\tepisode 266 finished after 123 steps\n",
            "\tepisode 267 finished after 134 steps\n",
            "\tepisode 268 finished after 99 steps\n",
            "\tepisode 269 finished after 147 steps\n",
            "\tepisode 270 finished after 184 steps\n",
            "\tepisode 271 finished after 168 steps\n",
            "\tepisode 272 finished after 42 steps\n",
            "\tepisode 273 finished after 157 steps\n",
            "\tepisode 274 finished after 81 steps\n",
            "\tepisode 275 finished after 200 steps\n",
            "\tepisode 276 finished after 200 steps\n",
            "\tepisode 277 finished after 101 steps\n",
            "\tepisode 278 finished after 200 steps\n",
            "\tepisode 279 finished after 200 steps\n",
            "\tepisode 280 finished after 169 steps\n",
            "\tepisode 281 finished after 82 steps\n",
            "\tepisode 282 finished after 176 steps\n",
            "\tepisode 283 finished after 200 steps\n",
            "\tepisode 284 finished after 200 steps\n",
            "\tepisode 285 finished after 195 steps\n",
            "\tepisode 286 finished after 157 steps\n",
            "\tepisode 287 finished after 73 steps\n",
            "\tepisode 288 finished after 142 steps\n",
            "\tepisode 289 finished after 200 steps\n",
            "\tepisode 290 finished after 187 steps\n",
            "\tepisode 291 finished after 195 steps\n",
            "\tepisode 292 finished after 40 steps\n",
            "\tepisode 293 finished after 182 steps\n",
            "\tepisode 294 finished after 37 steps\n",
            "\tepisode 295 finished after 46 steps\n",
            "\tepisode 296 finished after 155 steps\n",
            "\tepisode 297 finished after 134 steps\n",
            "\tepisode 298 finished after 181 steps\n",
            "\tepisode 299 finished after 55 steps\n",
            "14:06:06.982034\t\t...done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1LbxFqx6ca8"
      },
      "source": [
        "plot_durations([dqn.episode_durations], ['DQN'])"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_Zi8ZaF6cbZ"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4bVTzQKuIZ8"
      },
      "source": [
        "# III. Planning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0V5x-Axt22Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}